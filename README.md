# Embeddings & Clustering Workflow with pgvector

This repository provides a pipeline that:

1. Embeds support ticket text using Google Vertex AI.
2. Clusters the embedded vectors using KMeans.
3. Stores the embeddings and their associated metadata in PostgreSQL using the `pgvector` extension.
4. Provides user query support for data retrieval

---

## üîß Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

### 2. Install Python Dependencies

Use a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

Make sure your `requirements.txt` includes:
- `pandas`
- `sqlalchemy`
- `pgvector`
- `scikit-learn`
- `psycopg2`
- `google-cloud-aiplatform`

---

---

## üß† Pipeline Overview

### 1. Load Ticket Data

- You can pass a list of dictionaries (`List[Dict]`) or a `pandas.DataFrame` with `ticket_id` and `text` fields.

### 2. Generate Embeddings with Vertex AI

```python
embeddings = get_vertex_embedding(texts)
```

> Make sure to authenticate with Google Cloud SDK and enable Vertex AI API.

### 3. Perform KMeans Clustering

```python
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=5)
cluster_ids = kmeans.fit_predict(embeddings)
```

### 4. Store in PostgreSQL with pgvector

```python
new_ticket = Ticket(
    ticket_id=row['ticket_id'],
    text=row['text'],
    embedding=np.array(embedding, dtype=np.float32),
    cluster_id=cluster_ids[i]
)
session.add(new_ticket)
session.commit()
```

> Ensure the `embedding` column in your PostgreSQL table uses the `VECTOR(768)` type from pgvector.

---

## ‚öôÔ∏è PostgreSQL Table Setup

Make sure your table is created like this:

```sql
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE tickets (
    ticket_id VARCHAR PRIMARY KEY,
    text TEXT,
    embedding VECTOR(768),
    cluster_id INTEGER
);
```

---

## ‚úÖ Example Usage

```python
from embeddings_clustering import process_and_store_to_pgvector

kmeans = KMeans(n_clusters=5)
process_and_store_to_pgvector(kmeans, tickets_df=my_dataframe)
```

---

### 5. Optionally store in Qdrant DB. Use binaries for local server. Use qdrant setup file in db_connections folder

```python
    def create_db(self):
        print("Creating collection")
        self.client.recreate_collection(
        collection_name="support_tickets",
        vectors_config=VectorParams(size=3, distance=Distance.COSINE)  # use actual size of your embedding
    )
        self.client.upsert
        return self.client
```

### 6. Query the data
```
    vector serach in qdrant using client.search
    vector search in pgvector using manual sql queries as given in vertexAI_pipeline.ipynb
```


 
